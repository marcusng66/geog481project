{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Modular_CNN",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Network For Crop Prediction and Forecasting\n",
        "\n",
        "This script is for Group 12, Machine Learning Covolutional Neural Network by Gil and Marcus. "
      ],
      "metadata": {
        "id": "nRs11VxtboeE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries\n",
        "\n",
        "This first section of the script is used to import the various libraries that are required for the model and the statistics we need to be outputted at the end. "
      ],
      "metadata": {
        "id": "-bNrUGQ8DHU3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\r\n",
        "import datetime\r\n",
        "import csv\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n",
        "import IPython\r\n",
        "import IPython.display\r\n",
        "import matplotlib as mpl\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import seaborn as sns\r\n",
        "import tensorflow as tf\r\n",
        "import h5py\r\n",
        "from sklearn.metrics import r2_score\r\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "metadata": {
        "id": "5oTRCgiJC2L3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da026416-1fa7-45b0-cd84-cc623f652039"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constants and Global Variables\n",
        "\n",
        "This section is dedicated to determining the global variables that need to be carried over from the various modular components within the script. The Constants are the constants that we found worked best for creating the windows and training the models.\n"
      ],
      "metadata": {
        "id": "-x5hEcVsDMm2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# CONSTANTS\r\n",
        "\r\n",
        "OUT_STEPS = 19\r\n",
        "MAX_EPOCHS = 50\r\n",
        "\r\n",
        "# GLOBAL VARIABLES\r\n",
        "\r\n",
        "multi_val_performance = {}\r\n",
        "multi_performance = {}\r\n",
        "train_df = None\r\n",
        "val_df = None\r\n",
        "test_df = None\r\n",
        "num_features = []\r\n",
        "column_indices = []\r\n",
        "crop_type = []\r\n",
        "caruid = []\r\n",
        "history = []"
      ],
      "outputs": [],
      "metadata": {
        "id": "AVFMzAoyC-uZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract Usable Models\n",
        "\n",
        "This section is for extracting the various models that would work with the CNN. Through implementation we discovered that the CNN does not accept CSV files that contain less than around 350 rows. To remedy this problem, we queried all the available CSV files and put the names into their own file. This section queries that CSV file to retrieve all models that fulfill the 350 row requirement."
      ],
      "metadata": {
        "id": "C_GuH8wlp9aU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "usable_models = []\r\n",
        "usable_path = \"/content/drive/MyDrive/GEOG481/compiled/{0}usable.csv\".format(\"Sprwht\") # Replace Sprwhat with either Barley or Canola\r\n",
        "with open(usable_path, newline = '') as csvfile: \r\n",
        "  csvreader = csv.reader(csvfile, delimiter = ' ', quotechar = '|')\r\n",
        "  for rows in csvreader:\r\n",
        "    usable_models.append(rows[0].split(\",\")[0].split(\".\")[0])\r\n",
        "usable_models.pop(0) # Remove \"File Name\" from the rows"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File Name'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ],
      "metadata": {
        "id": "EoH0FwMOuf2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "79a2dff0-e48f-4f6c-b8d1-9fbf5bef75ff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Shaping and Manipulation\n",
        "\n",
        "The first section of the script involves taking the data from the CSV files and manipulating it to better fit into our CNN model. The first function, *data manipulation* is the manipulation of data that goes into our model. The second function, *create_dataset_X* is for configuring our data to be accepted for predictions."
      ],
      "metadata": {
        "id": "jSvvlLCgDQ05"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Function to split our data into training, validation and test data\r\n",
        "def data_manipulation(csv_file):\r\n",
        "  global multi_val_performance, multi_performance, train_df, val_df, test_df\r\n",
        "  global num_features, column_indices, crop_type, caruid, history\r\n",
        "\r\n",
        "  # This section assigns the correct CSV path and splits it to determine the current Crop Type and CARUID\r\n",
        "  csv_path = \"/content/drive/MyDrive/GEOG481/david_input/{0}_output_david/{1}.csv\".format(csv_file.split(\"_\")[0].lower().capitalize(), csv_file)\r\n",
        "  split_path = csv_path.split(\"/\")\r\n",
        "  csv_split = split_path[-1].split(\"_\")\r\n",
        "  crop_type, caruid = csv_split[0], csv_split[1].split(\".\")[0]\r\n",
        "\r\n",
        "  # Remove the unwanted value (\"Year\")\r\n",
        "  df = pd.read_csv(csv_path)\r\n",
        "  date_time = df.pop('Year')\r\n",
        "\r\n",
        "  # Plotting Function\r\n",
        "  plot_cols = ['Yield']\r\n",
        "  plot_features = df[plot_cols]\r\n",
        "  plot_features.index = date_time\r\n",
        "  _ = plot_features.plot(subplots=True)\r\n",
        "\r\n",
        "  column_indices = {name: i for i, name in enumerate(df.columns)}\r\n",
        "\r\n",
        "  # This part splits the data into different categories (70%, 20%, 10%)\r\n",
        "  n = len(df)\r\n",
        "  train_df = df[0:int(n*0.7)]\r\n",
        "  val_df = df[int(n*0.7):int(n*0.9)]\r\n",
        "  test_df = df[int(n*0.9):]\r\n",
        "\r\n",
        "  num_features = df.shape[1]\r\n",
        "\r\n",
        "  # This Section normalizes the data so that it is all scaled properly\r\n",
        "  train_mean = train_df.mean()\r\n",
        "  train_std = train_df.std()\r\n",
        "\r\n",
        "  train_df = (train_df - train_mean) / train_std\r\n",
        "  val_df = (val_df - train_mean) / train_std\r\n",
        "  test_df = (test_df - train_mean) / train_std\r\n",
        "\r\n",
        "  # Plotting function\r\n",
        "  df_std = (df - train_mean) / train_std\r\n",
        "  df_std = df_std.melt(var_name='Column', value_name='Normalized')\r\n",
        "  plt.figure(figsize=(12, 6))\r\n",
        "  ax = sns.violinplot(x='Column', y='Normalized', data=df_std)\r\n",
        "  _ = ax.set_xticklabels(df.keys(), rotation=90)\r\n",
        "\r\n",
        "# Function that creates a dataset for prediction\r\n",
        "def create_dataset_X(X, time_steps=1):\r\n",
        "    Xs = []\r\n",
        "    for i in range(len(X) - time_steps):\r\n",
        "        v = X.iloc[i:(i + time_steps)].values\r\n",
        "        Xs.append(v)\r\n",
        "    return np.array(Xs)"
      ],
      "outputs": [],
      "metadata": {
        "id": "m7XqOcZiDBFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the Window Generator\n",
        "\n",
        "This part of the script is used to create the WindowGenerator that acts as the backbone of the model. "
      ],
      "metadata": {
        "id": "qTMW1g8sE8PN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class WindowGenerator():\r\n",
        "  def __init__(self, input_width, label_width, shift,\r\n",
        "               train_df, val_df, test_df,\r\n",
        "               label_columns=None):\r\n",
        "    # Store the raw data.\r\n",
        "    self.train_df = train_df\r\n",
        "    self.val_df = val_df\r\n",
        "    self.test_df = test_df\r\n",
        "\r\n",
        "    # Work out the label column indices.\r\n",
        "    self.label_columns = label_columns\r\n",
        "    if label_columns is not None:\r\n",
        "      self.label_columns_indices = {name: i for i, name in\r\n",
        "                                    enumerate(label_columns)}\r\n",
        "    self.column_indices = {name: i for i, name in\r\n",
        "                           enumerate(train_df.columns)}\r\n",
        "\r\n",
        "    # Work out the window parameters.\r\n",
        "    self.input_width = input_width\r\n",
        "    self.label_width = label_width\r\n",
        "    self.shift = shift\r\n",
        "\r\n",
        "    self.total_window_size = input_width + shift\r\n",
        "\r\n",
        "    self.input_slice = slice(0, input_width)\r\n",
        "    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\r\n",
        "\r\n",
        "    self.label_start = self.total_window_size - self.label_width\r\n",
        "    self.labels_slice = slice(self.label_start, None)\r\n",
        "    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\r\n",
        "\r\n",
        "  def __repr__(self):\r\n",
        "    return '\\n'.join([\r\n",
        "        f'Total window size: {self.total_window_size}',\r\n",
        "        f'Input indices: {self.input_indices}',\r\n",
        "        f'Label indices: {self.label_indices}',\r\n",
        "        f'Label column name(s): {self.label_columns}'])\r\n",
        "\r\n",
        "def split_window(self, features):\r\n",
        "  inputs = features[:, self.input_slice, :]\r\n",
        "  labels = features[:, self.labels_slice, :]\r\n",
        "  if self.label_columns is not None:\r\n",
        "      labels = tf.stack(\r\n",
        "          [labels[:, :, self.column_indices[name]] for name in self.label_columns],\r\n",
        "          axis=-1)\r\n",
        "\r\n",
        "  # Slicing doesn't preserve static shape information, so set the shapes\r\n",
        "  # manually. This way the `tf.data.Datasets` are easier to inspect.\r\n",
        "  inputs.set_shape([None, self.input_width, None])\r\n",
        "  labels.set_shape([None, self.label_width, None])\r\n",
        "\r\n",
        "  return inputs, labels\r\n",
        "\r\n",
        "def plot(self, model=None, plot_col='Yield', max_subplots=3):\r\n",
        "  inputs, labels = self.example\r\n",
        "  plt.figure(figsize=(12, 8))\r\n",
        "  plot_col_index = self.column_indices[plot_col]\r\n",
        "  max_n = min(max_subplots, len(inputs))\r\n",
        "  for n in range(max_n):\r\n",
        "    plt.subplot(max_n, 1, n+1)\r\n",
        "    plt.ylabel(f'{plot_col} [normed]')\r\n",
        "    plt.plot(self.input_indices, inputs[n, :, plot_col_index],\r\n",
        "             label='Inputs', marker='.', zorder=-10)\r\n",
        "    if self.label_columns:\r\n",
        "      label_col_index = self.label_columns_indices.get(plot_col, None)\r\n",
        "    else:\r\n",
        "      label_col_index = plot_col_index\r\n",
        "\r\n",
        "    if label_col_index is None:\r\n",
        "      continue\r\n",
        "\r\n",
        "    plt.scatter(self.label_indices, labels[n, :, label_col_index],\r\n",
        "                edgecolors='k', label='Labels', c='#2ca02c', s=64)\r\n",
        "    if model is not None:\r\n",
        "      predictions = model(inputs)\r\n",
        "      plt.scatter(self.label_indices, predictions[n, :, label_col_index],\r\n",
        "                  marker='X', edgecolors='k', label='Predictions',\r\n",
        "                  c='#ff7f0e', s=64)\r\n",
        "\r\n",
        "    if n == 0:\r\n",
        "      plt.legend()\r\n",
        "\r\n",
        "  plt.xlabel('Time [3-Week Average]')\r\n",
        "\r\n",
        "def make_dataset(self, data):\r\n",
        "    data = np.array(data, dtype=np.float32)\r\n",
        "    ds = tf.keras.preprocessing.timeseries_dataset_from_array(\r\n",
        "        data=data,\r\n",
        "        targets=None,\r\n",
        "        sequence_length=self.total_window_size,\r\n",
        "        sequence_stride=1,\r\n",
        "        shuffle=True,\r\n",
        "        batch_size=32,)\r\n",
        "\r\n",
        "    ds = ds.map(self.split_window)\r\n",
        "    return ds\r\n",
        "\r\n",
        "@property\r\n",
        "def train(self):\r\n",
        "  return self.make_dataset(self.train_df)\r\n",
        "\r\n",
        "@property\r\n",
        "def val(self):\r\n",
        "  return self.make_dataset(self.val_df)\r\n",
        "\r\n",
        "@property\r\n",
        "def test(self):\r\n",
        "  return self.make_dataset(self.test_df)\r\n",
        "\r\n",
        "@property\r\n",
        "def example(self):\r\n",
        "  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\r\n",
        "  result = getattr(self, '_example', None)\r\n",
        "  if result is None:\r\n",
        "    # No example batch was found, so get one from the `.train` dataset\r\n",
        "    result = next(iter(self.train))\r\n",
        "    # And cache it for next time\r\n",
        "    self._example = result\r\n",
        "  return result\r\n",
        "\r\n",
        "WindowGenerator.train = train\r\n",
        "WindowGenerator.val = val\r\n",
        "WindowGenerator.test = test\r\n",
        "WindowGenerator.example = example\r\n",
        "WindowGenerator.plot = plot\r\n",
        "WindowGenerator.split_window = split_window\r\n",
        "WindowGenerator.make_dataset = make_dataset\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "risOy96TE-5b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compile and Fit Function\n",
        "\n",
        "This section is essentially the training procedure bottled into a single function.  "
      ],
      "metadata": {
        "id": "drj2NKRspC64"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def compile_and_fit(model, window, patience=5):\r\n",
        "  # Checkpoint callback usage\r\n",
        "  checkpoint_path = '/content/drive/MyDrive/GEOG481/output/{0}/Checkpoints/{1}/cp.ckpt'.format(crop_type, caruid)\r\n",
        "  # checkpoint_path = 'saved_model/Barley_1100/training_1/cp.ckpt'\r\n",
        "  \r\n",
        "  # Create a callback that saves the model's weights\r\n",
        "  cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\r\n",
        "                                                  save_weights_only=True,\r\n",
        "                                                  monitor='val_accuracy',\r\n",
        "                                                  verbose=1)\r\n",
        "  \r\n",
        "  # Changed from meansquarederror() to SparseCategoricalCrossentropy()\r\n",
        "  # optimizer changed from tf.optimizer.Adam() to 'adam'\r\n",
        "  model.compile(loss=tf.losses.MeanSquaredError(),\r\n",
        "                optimizer='adam',\r\n",
        "                metrics=[tf.metrics.MeanSquaredError()])\r\n",
        "\r\n",
        "  history = model.fit(window.train, epochs=MAX_EPOCHS,\r\n",
        "                      validation_data=window.val,\r\n",
        "                      callbacks=[cp_callback])\r\n",
        "  return history"
      ],
      "outputs": [],
      "metadata": {
        "id": "Yc9iqD79pHID"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Creation\n",
        "\n",
        "This is the function that creates the model itself and determines the early statistics for the initial model creation."
      ],
      "metadata": {
        "id": "Z4Ii2kZypSEc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def model_creation(): \r\n",
        "  # Create the window that will be used to help create the model\r\n",
        "  multi_window = WindowGenerator(input_width=19,\r\n",
        "                                label_width=OUT_STEPS,\r\n",
        "                                shift=OUT_STEPS,\r\n",
        "                                train_df=train_df,\r\n",
        "                                val_df = val_df,\r\n",
        "                                test_df = test_df)\r\n",
        "  print(multi_window)\r\n",
        "\r\n",
        "  CONV_WIDTH = 3\r\n",
        "  multi_conv_model = tf.keras.Sequential([\r\n",
        "      # Shape [batch, time, features] => [batch, CONV_WIDTH, features]\r\n",
        "      tf.keras.layers.Lambda(lambda x: x[:, -CONV_WIDTH:, :]),\r\n",
        "      # Shape => [batch, 1, conv_units]\r\n",
        "      tf.keras.layers.Conv1D(256, activation='relu', kernel_size=(CONV_WIDTH)),\r\n",
        "      # Shape => [batch, 1, out_steps*features]\r\n",
        "      tf.keras.layers.Dense(OUT_STEPS*num_features,\r\n",
        "                            kernel_initializer=tf.initializers.zeros()),\r\n",
        "      # Shape => [batch, out_steps, features]\r\n",
        "      tf.keras.layers.Reshape([OUT_STEPS, num_features])\r\n",
        "  ])\r\n",
        "\r\n",
        "  # Train the model once using compile_and_fit\r\n",
        "  history = compile_and_fit(multi_conv_model, multi_window)\r\n",
        "\r\n",
        "  IPython.display.clear_output()\r\n",
        "\r\n",
        "  # Check the performance of the new model using the evaluate function\r\n",
        "  multi_val_performance['Conv'] = multi_conv_model.evaluate(multi_window.val, verbose=1)\r\n",
        "  multi_performance['Conv'] = multi_conv_model.evaluate(multi_window.test, batch_size=32, verbose=1)\r\n",
        "  multi_window.plot(multi_conv_model)\r\n",
        "\r\n",
        "  # Save it in the pre-determined directory\r\n",
        "  multi_conv_model.save('/content/drive/MyDrive/GEOG481/output/{0}/Models/{1}/model'.format(crop_type, caruid))\r\n",
        "\r\n",
        "  print(\"Model Created\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "gSwy3hl1pQyk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training\n",
        "\n",
        "This section is dedicated to performing the model training. We found that the optimal training length for our models were 20 interations using 50 epochs."
      ],
      "metadata": {
        "id": "FWprdOAwpaoU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def model_training():\r\n",
        "  for i in range(20):\r\n",
        "    # Recreate the multi-window, same as model creation\r\n",
        "    multi_window = WindowGenerator(input_width=19,\r\n",
        "                                  label_width=OUT_STEPS,\r\n",
        "                                  shift=OUT_STEPS,\r\n",
        "                                  train_df = train_df,\r\n",
        "                                  val_df = val_df,\r\n",
        "                                  test_df = test_df)\r\n",
        "\r\n",
        "    # Load the model that was saved previously\r\n",
        "    new_model = tf.keras.models.load_model('/content/drive/MyDrive/GEOG481/output/{0}/Models/{1}/model'.format(crop_type, caruid))\r\n",
        "    # Train the model again\r\n",
        "    history = compile_and_fit(new_model, multi_window)\r\n",
        "    # Save the model again\r\n",
        "    new_model.save('/content/drive/MyDrive/GEOG481/output/{0}/Models/{1}/model'.format(crop_type, caruid))\r\n",
        "\r\n",
        "  # Plot the performance of the new model\r\n",
        "  multi_window.plot(new_model)"
      ],
      "outputs": [],
      "metadata": {
        "id": "P-CSfSwSpcoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Denormalize Predictions\n",
        "\n",
        "This is a helper function that is used to denormalize the predictions after the data is run through the predict function."
      ],
      "metadata": {
        "id": "H28qn1TZU-HH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def denormalize_pred(predictions, train):\r\n",
        "  new_values =[]\r\n",
        "  train_mean = train_df.mean() # Redetermine the training mean\r\n",
        "  train_std = train_df.std() # Redetermine the training std\r\n",
        "\r\n",
        "  for arrays in predictions:\r\n",
        "    for predict_vals in arrays:\r\n",
        "      denormalized_values = (predict_vals*train_std) + train_mean # Convert the value back to normal\r\n",
        "      new_values.append(denormalized_values)\r\n",
        "\r\n",
        "  return new_values"
      ],
      "outputs": [],
      "metadata": {
        "id": "R1Hcf7shU9en"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction\n",
        "\n",
        "The final technical function in the script is the one that retrieves predictions and denormalizes them for use and comparison. It will use the full dataset as its input and retrieve predictions for comparison."
      ],
      "metadata": {
        "id": "uGa3x9U-qVOa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def grab_predictions(file_name):\r\n",
        "  crop_type = file_name.split(\"_\")[0] \r\n",
        "  caruid = file_name.split(\"_\")[1].split(\".\")[0]\r\n",
        "  \r\n",
        "  direct = \"/content/drive/MyDrive/GEOG481/david_input/{0}_output_david/\".format(crop_type)\r\n",
        "  new_model = tf.keras.models.load_model('/content/drive/MyDrive/GEOG481/output/{0}/Models/{1}/model'.format(crop_type, caruid))\r\n",
        "\r\n",
        "  # Retrieve the correct CSV for the model retrieved previously\r\n",
        "  df = pd.read_csv(direct + \"/\" + file_name + \".csv\")\r\n",
        "  df.pop(\"Year\")\r\n",
        "\r\n",
        "  # Recreate the same training data as before\r\n",
        "  n = len(df)\r\n",
        "  train_df = df[0:int(n*0.7)]\r\n",
        "\r\n",
        "  # Normalize data once again\r\n",
        "  train_mean = train_df.mean()\r\n",
        "  train_std = train_df.std()\r\n",
        "\r\n",
        "  df = (df - train_mean) / train_std\r\n",
        "\r\n",
        "  # Create a valid dataset and retrieve predictions\r\n",
        "  time_steps = 19\r\n",
        "  X_full_data = create_dataset_X(df, time_steps)\r\n",
        "  df_pred = new_model.predict(X_full_data)\r\n",
        "\r\n",
        "  # Normalize and pull yields out of the .predict output\r\n",
        "  normalized_pred = denormalize_pred(df_pred, train)\r\n",
        "  yield_predictions = []\r\n",
        "  for predictions in normalized_pred:\r\n",
        "    yield_predictions.append(predictions[7])\r\n",
        "\r\n",
        "  # Find the average of every 19 window steps that was used in model creation\r\n",
        "  all_predictions = []\r\n",
        "  for i in range(0, 627):\r\n",
        "    current_predictions = yield_predictions[i*19: (i+1)*19]\r\n",
        "    all_predictions.append(np.mean(current_predictions))\r\n",
        "\r\n",
        "  # Add predictions to the CSV and save it to a new output file\r\n",
        "  df = pd.read_csv(direct + \"/\" + f)\r\n",
        "  df = df[19:]\r\n",
        "  df[\"Predict\"] = all_predictions\r\n",
        "\r\n",
        "  #Create an output folder named [cropname]_predict, change directory here\r\n",
        "  df.to_csv(\"/content/drive/MyDrive/GEOG481/output/{0}/Predict/{1}_PREDICT.csv\".format(crop_type,file_name.split(\".\")[0]), index=False)"
      ],
      "outputs": [],
      "metadata": {
        "id": "2xzRGj4YqUaz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Creation Iteration\n",
        "\n",
        "This is the section used for iterating through all usable models and creating the models for each CARUID and crop type. Commented out so that new models aren't constantly recreated."
      ],
      "metadata": {
        "id": "yulNSPFiFBmh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# for file in usable_models:\r\n",
        "#   print(file)\r\n",
        "#   data_manipulation(file) # Manipulate Data\r\n",
        "#   model_creation() # Create Models\r\n",
        "  "
      ],
      "outputs": [],
      "metadata": {
        "id": "QrUzmeO_DU9j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training Iteration\n",
        "\n",
        "This is the section used for iterating through all usable models and training the models for each CARUID and crop type. Commented out so that training isn't performed when it is not desired."
      ],
      "metadata": {
        "id": "V9b19tvyq5qi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# for file in usable_models: \n",
        "#     print(file) \n",
        "#     data_manipulation(file) # Manipulate Data\n",
        "#     model_training() # Train Models"
      ],
      "outputs": [],
      "metadata": {
        "id": "Jci0Qk1Rp-FQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Prediction Iteration\n",
        "\n",
        "This is the section used for iterating through all usable models and grabbing the predicted values from their corresponding models for each CARUID and crop type. Commented out so that predictions aren't acquired when it is not desired."
      ],
      "metadata": {
        "id": "gltpo4oLqbSi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# for file in usable_models:\n",
        "#   print(file)\n",
        "#   grab_predictions(file) # Grab predictions"
      ],
      "outputs": [],
      "metadata": {
        "id": "1w_seVVXQ-Ns"
      }
    }
  ]
}